{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f82ddcf",
   "metadata": {},
   "source": [
    "# Vectorization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc55b45",
   "metadata": {},
   "source": [
    "Count based Vectorization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225232f0",
   "metadata": {},
   "source": [
    "# Bag of Words \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe04d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sourav Mondal\\Desktop\\Practice Coding\\Marketing Agent Framework\\src\\core\n"
     ]
    }
   ],
   "source": [
    "# Create a corpus of all unique words \n",
    "# Count the Frequecy of the words in the Corpus one by one in a sentence \n",
    "# Limitation - 1) Big Sparse Vector(fill with lot of Zeros), 2) Can't Capture Semantic Meaning\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "data = pd.read_csv(\"../../sample_reviews.csv\")\n",
    "review_data = data[\"review_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e36146",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scratch Coding \n",
    "class Bag_of_words():\n",
    "    def build_vocabulary(self,texts):\n",
    "        vocab = {}\n",
    "        idx = 0\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = idx\n",
    "                    idx += 1\n",
    "        return vocab\n",
    "    def text_to_bow_vector(self,text, vocab):\n",
    "        vector = [0] * len(vocab)\n",
    "        for word in text.split():\n",
    "            if word in vocab:\n",
    "                vector[vocab[word]] += 1 \n",
    "        return vector\n",
    "    def vectorize(self, review_data):\n",
    "        vocab=self.build_vocabulary(texts=review_data)\n",
    "        vectors = []\n",
    "        for text in review_data:   \n",
    "            vector=self.text_to_bow_vector(text=text, vocab=vocab)\n",
    "            vectors.append(vector)\n",
    "        return vectors \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c842ca9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Using Python Library \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer()\n",
    "data = bow.fit_transform(review_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdea9dc",
   "metadata": {},
   "source": [
    "# TD-IDF = TF(Term Frequency) * IDF(Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33c7c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tf_idf():\n",
    "    def fit_transform(self,review_data):\n",
    "        docs = []\n",
    "        for doc in review_data:\n",
    "            for element in doc.lower().split():\n",
    "                docs.append(element)\n",
    "\n",
    "        vocab = []\n",
    "        for item in docs:\n",
    "            if item not in vocab:\n",
    "                vocab.append(item)\n",
    "\n",
    "        # Calculate TF\n",
    "        # TF = count of term in document / total words in document \n",
    "        tf = []\n",
    "        for sentence in review_data:\n",
    "            line = sentence.lower().split()\n",
    "            tf_doc = {}\n",
    "            for word in vocab:\n",
    "                tf_doc[word] = line.count(word) / len(line)\n",
    "            tf.append(tf_doc)\n",
    "\n",
    "        # Calculate IDF \n",
    "        # IDF = log(len(docs) + 1/ DF(t) + 1) + 1\n",
    "        # DF = number of Documents containing term t \n",
    "        df = {}\n",
    "        for word in vocab:\n",
    "            count = 0\n",
    "            for sentence in review_data:\n",
    "                line = sentence.lower().split()\n",
    "                if word in line:\n",
    "                    count += 1\n",
    "            df[word] = count  \n",
    "        import math \n",
    "        idf = {}\n",
    "        for word in vocab:\n",
    "            idf[word] = math.log((len(review_data) + 1) / (df[word] + 1)) + 1\n",
    "\n",
    "        # TF * IDF \n",
    "        tfidf = []\n",
    "        for tf_ind in tf:\n",
    "            tf_idf_doc = {}\n",
    "            for word in vocab:\n",
    "                tf_idf_doc[word] = tf_ind[word] * idf[word]\n",
    "            tfidf.append(tf_idf_doc)\n",
    "        \n",
    "          \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47ba2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = tf_idf()\n",
    "data = vectorizer.fit_transform(review_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ac7dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "data = vectorizer.fit_transform(review_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
